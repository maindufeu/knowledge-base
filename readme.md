![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2FAI-ML-Lab%2Fresources%2Fmain%2Fproperties.json&query=%24.repo.ac_llm%5B0%5D.version&logo=bitbucket&label=version&labelColor=1A3B47) ![version from json](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2FAI-ML-Lab%2Fresources%2Fmain%2Fproperties.json&query=%24.repo.ac_llm%5B%3F(%40.version%20%3D%3D%20'0.1.0')%5D.usos&label=usos%20en%20proyecto&labelColor=1A3B47) ![website](https://img.shields.io/website/http/ailyn.link?&labelColor=1A3B47&label=demo) [![Email me](https://img.shields.io/badge/Email%20me-1A3B47)](mailto:eejimenez@minsait.com) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1EgAr7ZjzZe1DdlqQsHluX20i5GCZu_rV?usp=sharing) ![language](https://img.shields.io/badge/language-python-blue)
[![Download as PDF Button](https://img.shields.io/badge/Download%20%20pdf-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24)](https://mdtopdf.up.railway.app/convertPdf?url=https://raw.githubusercontent.com/AI-ML-Lab/resources/main/Readme.md)
<p align="center">
<img src="images/wh.png"width=20%>
<p align="center" href=>
    <img src="images/m_dimensiones.gif"width=7%>
    <img src="images/m_lab.png" width=30%>
<p align="center">
<img src="images/wh.png"width=10%>

![banner](images/m_banner-acllm.png)

<a name="toc"></a>


[![benchmark](images/card_benchmark.png)](#bench) [![seguridad](images/card_seguridad.png)](#sec) [![licencias](images/card_licencias.png)](#lic)[![integraciones](images/card_integraciones.png)](#int) [![costos](images/card_costos.png)](#cost) [![referencias](images/card_referencias.png)](#ref) [![referencias](images/card_tiempo.png)](#acel)

<a name="ap"></a>
## **Aplicaciones**

Las aplicaciones de este acelerador se dividen en e 3:

- **Out of the box**: Ejemplos m√≠nimos que ayuden a entender las capacidades de los modelos de lenguajes en diferentes frameworks 
- **Casos particulare**s (Demos): asistentes inteligentes que pueden responder preguntas a trav√©s de b√∫squeda en bases de datos vectoriales construidas a partir de datos estructurados (SQL,CSV, txt, PDF, jsons) 
- **Herramientas**: Interfaces y funciones que ayudan a la implementaci√≥n de proyectos relacionados

### Out of the box [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/10VJRNKdt9cGKIgtyxb0yWDdVVkPbhEX-?usp=sharing)

- Traducci√≥n
- An√°lisis de sentimiento
- Resumen
- Par√°frasis
- Correcci√≥n sint√°ctica
- Evaluaci√≥n de reglas

![Alt text](images/ref_t5-capabilities.png){width=500}

### Casos de uso particulares
     

![MIA](images/ap_mia.png){width=30 height=30} **Asistente para planeaci√≥n de viajes y reservaciones** [![try me](https://img.shields.io/badge/Try%20me-Whatsapp-brightgreen?logo=whatsapp)](https://api.whatsapp.com/send/?phone=5215573747795&text&type=phone_number&app_absent=0) [![repo](https://img.shields.io/badge/demo-blue?logo=bitbucket)](https://bitbucket.indra.es/projects/GT_PLAIMX/repos/de_mia-langchain) 

![aeromexico](images/ap_aeromexico.png){width=30 height=30} **Recuperaci√≥n de informaci√≥n de pasajeros** [![repo](https://img.shields.io/badge/proyecto-blue?logo=bitbucket)](https://bitbucket.indra.es/projects/GT_PLAIMX/repos/pr_aeromexico-pronostico/browse)


### Herramientas [![repo](https://img.shields.io/badge/proyecto-blue?logo=bitbucket)](https://bitbucket.indra.es/projects/GT_PLAIMX/repos/de_llm)

Aplicaci√≥n para entrenamiento semi supervisado de modelos de lenguaje

<a name="bench"></a>
## Benchmark
[:arrow_up:](#toc)

Si bien no existe una metodolog√≠a de comparaci√≥n de LLMs dominante o probada existen algunos referentes en la indsutria que nos ayudan a tener una idea del performance general que llevan los modelos que se van generando evaluandolos en diferentes tareas. Para esto el que mantiene m√°s actividad por ser una evaluaci√≥n autom√°tica es [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval/tree/main) y [MosaicML](https://www.mosaicml.com/llm-evaluation) puedes ver su [score](https://docs.google.com/spreadsheets/d/e/2PACX-1vSAwA8-DdEgGHlbX1XkP7KoYWQD2HzKDGsID33MypM17FsjVw5YmT4ceUK-ryfH4jL9jBW8u1DTGWuS/pubhtml#) en tiempo real.

<p align="center">
    <img src="images/ref_mosaic-eval.png" width=40%>
    <img src="images/ref_benchmark.png" width=30%>

### Performance
Sin embargo podemos evaluar lo siguiente a grandes rasgos:

- **Conocimiento del Mundo:** capacidad para responder preguntas sobre una amplia variedad de temas con exactitud.

- **Sentido Com√∫n:** capacidad de realizar tareas de razonamiento b√°sico e inferencias utilizando conocimiento del sentido com√∫n.

- **Comprensi√≥n del Lenguaje:** comprensi√≥n de los modelos en idiomas, traducciones y resoluci√≥n de an√°foras.

- **Resoluci√≥n de Problemas Simb√≥licos:** resoluci√≥n de diversas tareas como aritm√©tica, l√≥gica y algoritmos.

- **Comprensi√≥n de Lectura:** Precisi√≥n de respuestas a preguntas basadas en textos proporcionados.

- **Programaci√≥n:** Evaluaci√≥n de la comprensi√≥n y generaci√≥n de c√≥digo funcional por parte de los modelos.

En funci√≥n del caso de uso lo m√°s recomendable es usar m√©tricas personalizadas que incluyan la proporci√≥n de aciertos al realizar la tarea predefinida en el template,o la tasa de alucinaciones que son generadas o incluso independientemente de la respuesta del modelo y sus posibles alucinaciones el n√∫mero de veces que se complet√≥ la tarea satisfactoriamente usando un modelo de evaluaci√≥n con otro llm o con una calificaci√≥n humana que sirva de reforzamiento.

Un ejemplo de caso de uso es, si la tarea es generar una query que permita hacer una consulta sobre un nombre o un lugar en particular el n√∫mero de fallas que se registraron al hacer esas querys


## Modelo elegido para acelerar

**Llama2** ü¶ô

<a name="lic"></a>
## Licenciamiento
[:arrow_up:](#toc)

Los modelos de Llama est√°n disponibles bajo una licencia que permite su uso en proyectos comerciales sin embargo no es estrictamente Open Source.

Llama 2 est√° licenciado bajo la LLAMA 2 Community License, Copyright (c) Meta Platforms, Inc. que se resume en lo siguiente:

1. **Derechos de redistribuci√≥n**: Es una licencia no exclusiva, mundial, no transferible y libre de regal√≠as para usar, reproducir, distribuir, copiar, crear obras derivadas y modificar los Materiales de Llama
2. **Terminos adicionales**: si los usuarios activos son mayores a 700 millones al mes se debe pedir un permiso a Meta para hacer uso de Llama y sus derivados
3. **Garant√≠a**: No posee ninguna garant√≠a de funcionamiento
4. **No responsabilidad**: En caso de negligencia, da√±o o mal uso Meta no se responsabiliza
5. **Propiedad intelectual**: Los productos derivados de Llama son propiedad de quien los crea

Por lo tanto, la licencia de Meta para los modelos y c√≥digo de LLaMa no cumple con este est√°ndar de OSD; espec√≠ficamente, pone restricciones al uso comercial para algunos usuarios y tambi√©n restringe el uso del modelo y software para ciertos prop√≥sitos (la Pol√≠tica de Uso Aceptable).

## Relevancia del OSD

* Un licencia de c√≥digo abierto garantiza que los desarrolladores y usuarios puedan decidir por s√≠ mismos c√≥mo y d√≥nde usar la tecnolog√≠a sin necesidad de interactuar con otra parte; tienen soberan√≠a sobre la tecnolog√≠a que usan.
* El c√≥digo abierto se basa en el entendimiento de que todos pueden compartir sin importar qui√©n seas.

<a name="int"></a>
## Integraciones
[:arrow_up:](#toc)

Algunos frameworks disponbiles para trabajar con LLMs son:

| Proyecto        | Descripci√≥n                                                                                                 |
|-----------------|-------------------------------------------------------------------------------------------------------------|
| [Alpaca Farm ](https://github.com/tatsu-lab/alpaca_farm/)    | AlpacaFarm es un framework de reforzamiento humana. Contiene c√≥digo para simular retroalimentaci√≥n de preferencias, evaluaci√≥n automatizada y algoritmos de aprendizaje por refuerzo como PPO.                                                                                                                             |
| [Flax](https://flax.readthedocs.io/en/latest/)            | Flax es una biblioteca de redes neuronales para JAX que proporciona operaciones de diferenciaci√≥n y vectorizaci√≥n en python con el que se han construido modelos de lenguaje visi√≥n y multimodales como Palm                                                                                                           |
| [GGML](https://github.com/ggerganov/ggml)            | GGML es framwork para cuantizar LLMs y hacerlos m√°s ligeros. Bibliotecas populares basadas en GGML incluyen llama.cpp y whisper.cpp.                                                                                               |
| [Hugging Face](https://huggingface.co/models?other=LLM)    | Ofrece una amplia biblioteca de Transformers y Demos de LLMs para probar la mayor√≠a de ellos.                                                                                                          |
| [Lamini](https://github.com/lamini-ai/lamini)          |Permite el ajuste fino, RLHF y optimizaciones para que el LLM autohospedado funcione de manera eficiente.                                                                                                                     |
| [LangChain](https://python.langchain.com/docs/get_started/introduction.html)       | LangChain proporciona m√≥dulos para modelos, est√≠mulos, √≠ndices, cadenas y agentes, con la capacidad de "encadenar" estos m√≥dulos entre s√≠.                                                                                                                           |
| [LlamaIndex](https://github.com/jerryjliu/llama_index)      | Permite integrar datos privados/personalizados con conectores de datos, √≠ndices/gr√°ficos compatibles con LLM y una interfaz de consulta sobre los datos.                                                                                                                                     |
| [LMFlow](https://github.com/OptimalScale/LMFlow/blob/main/readme/README_es.md)          | Framework de integraci√≥n y ajuste de LLM y visi√≥n orientado a tareas. Soporta estructuras comunes como LLaMA y GPT-2.                                                                                                                                         |
| [MLC LLM](https://github.com/mlc-ai/mlc-llm)         | MLC LLM permite que se desplieguen LLMs nativamente en hardware y aplicaciones como en iOS, Android, Apple Silicon, AMD, Intel, NVIDIA y WebGPU.                                                             |


Dependiendo de la aplicaci√≥n podemos optar por diferentes plataformas y arquitecturas sin embargo podemos encontrar algunos patrones en la construcci√≥n de aplicaciones con LLMs que podemos clasificar como en el siguiente diagrama

<p align="center">
    <img src="images/ref_patterns.png" width=50%>

- Evals: m√©tricas de performance
- [RAG](https://learn.microsoft.com/en-us/azure/machine-learning/concept-retrieval-augmented-generation?view=azureml-api-2): (Retrieval Augmented Generation) agregar informaci√≥n reciente
- Fine-tuning: mejorar en tareas espec√≠ficas
- Caching: almacenamiento para reducir costos y latencia
- Guardrails: estrategias de contensi√≥n para mejorar la calidad de las salidas
- Defensive UX: Anticipar y manejar posibles errores desde el UX
- Collect user feedback: Generar un sistema de recomendaci√≥n

Hasta ahora s√≥lo se ha probado la integraci√≥n con langchain por su facilidad para hacer estrategias de cach√©, ux defensivo y filtros de calidad, tambi√©n debido a la documentaci√≥n y facilidad de integraci√≥n temprana sin embargo a 6 meses de la presentaci√≥n de chatGPT ya hay herramientas de bajo costo para hacer fine tuning. Esto puede traer los siguientes beneficios:

- **Rendimiento y control**: mejorando el rendimiento de un modelo base listo para usar e incluso superar a un LLM de terceros. Tambi√©n proporciona un mayor control sobre el comportamiento del LLM, lo que resulta en un sistema o producto m√°s robusto. En general, el fine-tuning nos permite crear productos que se diferenciadores.
- **Modularizaci√≥n**: El fine-tuning de una sola tarea nos permite utilizar un ej√©rcito de modelos m√°s peque√±os que se especializan cada uno en sus propias tareas. como moderaci√≥n de contenido, extracci√≥n, resumen, etc. Adem√°s, dado que cada modelo solo tiene que centrarse en un conjunto de tareas limitado, hay menos preocupaci√≥n de que el fine-tuning de un modelo en una tarea reduce el rendimiento en otras tareas.
- **Reducci√≥n de dependencias**: Al alojar nuestros propios modelos, podemos reducir las preocupaciones legales sobre los datos de propiedad (por ejemplo, PII, documentos internos y c√≥digo) que se exponen a las API externas y evita las restricciones que vienen con los LLM de terceros, como el l√≠mite de velocidad, los altos costos o los filtros de seguridad demasiado restrictivos. 

Este es el esquema b√°sico de construcci√≥n de aplicaciones en cach√©
<p align="center">
    <img src="images/ref_gptcache.jpg" width=50%>

Sin embargo para mejorar la precisi√≥n del modelo tenemos algunas opciones:

- Hacer fine-tunning de un modelo existente entren√°ndolo con data propia
- Cuantizar un modelo m√°s grande que ayude a mantener el uso de recursos y tiempo con una mayor precisi√≥n
- Usar un sheperd intermedio que haga una evaluaci√≥n de la respuesta final y la refine
- Mejorar nuestra b√∫squeda intermedia en la base de datos vectorial 

Si el modelo usado es local y hay tiempo en el proyecto un fine tuning podr√≠a parecer la opci√≥n m√°s evidente sin embargo es un proyecto en si mismo y es necesario dedicar teimpo a validar extensivamente el dataset con el que se va a ajustar el modelo siendo una opci√≥n s√≥lo para casos particulares donde ning√∫n modelo preentrenado tenga un dominio espec√≠fico sobre el tema y este no pueda ser contenido por la b√∫squeda en la base de datos vectorial.

En caso de que no seea una posibilidad modificar el modelo podemos optar por cambiar de motor de b√∫squeda o generar un sheperd que ayude a refinar las respuestas.

Algunos detalles sobre los sheperds disponibles pueden revisarse [aqu√≠](https://arxiv.org/pdf/2308.04592.pdf).

Algunas opciones de b√∫squeda de datos vectorial que hemos usado son [FAISS(META)](https://ai.meta.com/tools/faiss/), [Chroma](https://www.trychroma.com/), [Pinecone](https://www.pinecone.io/) y [Qdrant](https://qdrant.tech/).

Algunas de las diferencias esenciales son:

| | Chroma    | Pinecone            | FAISS | Qdrant | 
|- |---------|-----------------------|--|--|
| PROS | Facilidad de uso y deployment | Plataforma multiherramienta  |Facilidad de uso|Velocidad de b√∫squeda|
| CONTRAS | Performance en los resultados | Precio  |Tiempo de b√∫squeda|Es necesario levantar una API de consumo|

Es posible ver la implementaci√≥n de la API de consumo en Quadrant [aqu√≠](https://glowing-space-trout-p7976jg5w6qc7qxg.github.dev/)

Es posible ver un ejemplo de uso de chroma [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1EgAr7ZjzZe1DdlqQsHl1X20i5GCZu_rV?usp=sharing)



<a name="esc"></a>
## Escalabilidad
[:arrow_up:](#toc)

Dado que los Llms los podemos pensar en dos sentidos de integraci√≥n ahondaremos enn las posibilidades de escalar ambos escenarios.

En el primero hacemos un uso de la API de OpenAI con los siguientes rate limits:

<p align = "center">
    <img src="images/ref_ratelimit-openai.png"width=30%>

para los modelos m√°s antiguos tenemos las siguientes equivalencias de tokens:

| Type    | 1TPM                  |
|---------|-----------------------|
| davinci | 1 token per minute    |
| curie   | 25 tokens per minute  |
| babbage | 100 tokens per minute |
| ada     | 200 tokens per minute |

Estas limitaciones son a nivel organizaci√≥n y no a nivel usuario o key por lo que para un proyecto que requiera llevarse a productivo considerando conversaciones coocurrentes ser√≠a necesario utilizar varios modelos para evitar saturar los rate limits con pocos usuarios. Un escenario de ejemplo podr√≠a considerarse el siguiente:

Para una consulta en el asistente **MIA** se utilizan 3 requests a la API de OpenAI:

1. Una para determinar el comportamiento esperado y enrutar al agente necesario.
2. Otra para refrasear el input original para devolver un input que pueda ser comprendido por la API de Maps
3. Una tercera que dado el contexto devuelto por la API y el contexto de la conversaci√≥n responda adecuadamente a la pregunta

> Esto es significa que para responder una sola pregunta utiliza entre 500 y 1000 tokens dependiendo de la longitud del contexto. Por lo tanto una interacci√≥n com√∫n con MIA que puede requerir varias consultas o preguntas usa entre 500 hasta 10,000 tokens y unas 30 requests por usuario. Es claro que dado que los tiempos de latencia y escritura de los usuarios finales el n√∫mero de consultas por minuto puede variar pero si tom√°ramos una sola consulta por usuario al mismo tiempo llegar√≠amos al l√≠mite con 500 usuarios en el escenario m√°s favorable


$700tokens \times 500users = 350,000TPM$

En este escenario hay dos posibilidades para facilitar su escalabilidad, la primera es utilizar una variedad de modelos para finalidades distintas as√≠ podr√≠amos aprovechar el rate limit por modelo, el siguiente ser√≠a utilizar un modelo propio como Llama2 y una tercera opci√≥n ser√≠a encolar los usuairos simnultaneos para poder distribuir el n√∫mero de llamadas
>:warning: La integraci√≥n con Composer o alg√∫n pipeline basado en DAGs no se recomienda en ninguno de los dos casos como una implementaci√≥n punta a punta sino s√≥lamente para el proceso de reentrenamiento ya que al hacer un escalamiento din√°mico horizontal la carga de los modelos y en caso de requerirlo el entrenamiento se har√≠a en cada pod de procesamiento lo cu√°l implicar√≠a un costo de entrenamiento duplicado y tiempos m√°s largos de inferencia. Es preferible en todo caso para esta tarea un escalado vertical siempre y cuando se cuente con un modelo propio cargado en local


### Arquitecturas

La demo de MIA est√° montada en flask en una instancia de **EC2** que apunta a un dominio en **route53** con un certificado SSL de **ACM** ya que la API de whatsapp necesita que la informaci√≥n que va al webhook viaje a trav√©s de una conexi√≥n https. Para escalar esta demo no s√≥lo tenemos rate limits de consumo en las APIs sino limitantes de tr√°fico en una instancia **EC2**. Las pruebas que se han generado con Apache Benchmark nos dicen que un servidor de ec2 puede manejar 250 llamadas concurrentes con un tiempo promedio de respuesta de 86 milisegundos sin embargo deja de responder ante 450 llamadas concurrentes.



<p align = "center"><strong>Comparativa de arquitecturas</strong>
<p align = "center">
    <img src="images/arq_demo-mia.png"width=20%>
    <img src="images/wv.png"width=5%>
    <img src="images/arq_elb.png" width=30%>

<p align = "center">
    <img src="images/screen_loadtest-ec2.png"width=29%>
    <img src="images/wv.png"width=5%>
    <img src="images/screen_loadtest-elb.png" width=30%>


Al colocar un balanceador de carga observamos que las m√©tricas de respuesta se conservan sin embargo con dos instancias se pueden manejar hasta 750 llamadas concurrentes por el balanceador de carga.

Esto es muy parecido a la propuesta de una arquitectura de beanstalk toma como ejemplo de buena arquitectura para una aplicaci√≥n de flask en su [documentaci√≥n](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html) 

<p align="center">
    <img src="images/arq_beanstalk.png">

Sin embargo algunas alternativas de escalabilidad y portabilidad con un servicio de IaC puede ser un despliegue con **AWS CDK** y **FastAPI** como la de abajo que tambi√©n explica detalladamente aws en sus ejemplos de [well architecture](https://aws.amazon.com/es/blogs/machine-learning/deploy-a-serverless-ml-inference-endpoint-of-large-language-models-using-fastapi-aws-lambda-and-aws-cdk/). Esta ademas de ser muy portable tiene la bondad de que cada conversaci√≥n levanta una √∫nica funci√≥n lambdas que tiene sirve de enrutador para un modelo largo de lenguaje hacia las consultas a diferentes APIs y cumple con requerir poca RAM y poco tiempo de procesamiento por llamada. haciendo que la escalabilidad sea mayor por dise√±o y m√°s econ√≥mico en costos de servidor

<p align="center">
    <img src="images/arq_aws-cdk.png">

La elecci√≥n de la arquitectura a utilizar ser√° en funci√≥n de las necesidades de escalabilidad, familiaridad con la infraestructura y tiempo de implementaci√≥n y configuraci√≥n necesarias

<a name="sec"></a>
## Seguridad
[:arrow_up:](#toc)

El modelo Llama2 con sus pesos puede descargarse en local por lo que es posible aislar el componente de lenguaje del tr√°fico de Open-Web facilitando ajustarse a los lineamientos de ciberseguridad del cliente. 
No se han detectado vulnerabilidades comunes en la librer√≠a de langchain aunque si ineficiencias en el uso de los tokens ya que para hacer uso de la memoria hace un almacenamiento recursivo que usa como contexto y en conversaciones largas puede afectar seriamente el funcionamiento y los recursos de c√≥mputo disponibles

### Research & References

Llama 2

> :heavy_check_mark: Se han descargado los modelos llama7b y el fine tuning de llama2 7b-chat as√≠ como el modelo Llama2 70b-chat es posible revisar mayor informaci√≥n en la [p√°gina oficial](https://ai.meta.com/llama/)
  
<a name="cost"></a>
## Costo de implementaci√≥n
[:arrow_up:](#toc)


### Tiempo de desarrollo 

2 meses

### Tiempo ahorrado en creaci√≥n de demos

1 mes

<a name="ref"></a>
#### Art√≠culos
[:arrow_up:](#toc)

[Sharepoint](https://indra365.sharepoint.com/:f:/r/sites/Grp_T_Hub_AAI_LatAm-AILab/Documentos%20compartidos/Art%C3%ADculos?csf=1&web=1&e=dPLezJ)



<p align="center">
    <img src="images/m_bottom.png"
    width=95%>

<a name="repo"></a>
## √Årbol de repositorio
[:arrow_up:](#toc)
```
‚îÇ   image.png
‚îÇ   Readme.md
‚îÇ
‚îú‚îÄ‚îÄ‚îÄimages
‚îÇ       ap_aeromexico.png
‚îÇ       ap_mia.png
‚îÇ       arq_aws-cdk.png
‚îÇ       arq_beanstalk.png
‚îÇ       arq_demo-mia.png
‚îÇ       arq_elb.png
‚îÇ       card_benchmark.png
‚îÇ       card_costos.png
‚îÇ       card_integraciones.png
‚îÇ       card_licencias.png
‚îÇ       card_referencias.png
‚îÇ       card_seguridad.png
‚îÇ       card_tiempo.png
‚îÇ       m_ailab.png
‚îÇ       m_banner-acllm.png
‚îÇ       m_bottom.png
‚îÇ       m_dimensiones.gif
‚îÇ       m_lab.png
‚îÇ       ref_benchmark.png
‚îÇ       ref_gptcache.jpg
‚îÇ       ref_mosaic-eval.png
‚îÇ       ref_patterns.png
‚îÇ       ref_ratelimit-openai.png
‚îÇ       screen_loadtest-ec2.png
‚îÇ       screen_loadtest-elb.png
‚îÇ       wh.png
‚îÇ       wv.png
‚îÇ
‚îî‚îÄ‚îÄ‚îÄnotebooks
        Fine_tune_a_language_model.ipynb
        llm metrics.ipynb
```